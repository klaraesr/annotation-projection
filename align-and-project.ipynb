{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.core.display import display\n",
    "from deep_translator import GoogleTranslator\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.sv import Swedish\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "nlp_en = English()\n",
    "nlp_sv = Swedish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_pandas_display_options() -> None:\n",
    "    \"\"\"Set pandas display options.\"\"\"\n",
    "    # Ref: https://stackoverflow.com/a/52432757/\n",
    "    display = pd.options.display\n",
    "\n",
    "    display.max_columns = 100\n",
    "    display.max_rows = 10\n",
    "    display.max_colwidth = 200\n",
    "    display.width = 2000\n",
    "    # display.precision = 2  # set as needed\n",
    "\n",
    "set_pandas_display_options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print history of notebook\n",
    "# %history -g\n",
    "\n",
    "pkl_src_path = '../data/dataframes/coref_ontonotes_test.pkl'\n",
    "pkl_dest_path = '../data/dataframes/coref_with_translations_test.pkl'\n",
    "parallel_text_path = '../data/alignment/ontonotes/parallel_ontonotes_test.en-sv'\n",
    "alignment_sym_path = '../data/alignment/ontonotes/ontonotes_test.sym'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# I suc core är datan uppdelad i meningar redan (all conll?), så det behövs inte göras.\n",
    "df = pd.read_pickle(pkl_src_path)\n",
    "#df = pd.read_pickle(pkl_dest_path)\n",
    "texts = df['Text']\n",
    "all_tokens_org = []\n",
    "all_tokens_trans = []\n",
    "sentence_lengths_dict = df['Sentence_lengths']\n",
    "print('hej')\n",
    "display(df)\n",
    "#display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Deep translator takes a long time to run as it sleeps 2 seconds between every request\n",
    "with open(parallel_text_path, 'w') as writer:\n",
    "    for sentences in texts:\n",
    "        for tokens_org in sentences:\n",
    "            sentence = ' '.join(tokens_org)\n",
    "            try:\n",
    "                #translated_sents = GoogleTranslator('en', 'sv').translate_batch(joined_sents)\n",
    "                translated_sent = GoogleTranslator(source='en', target='sv').translate(text=sentence)\n",
    "                spacy_doc_sv = nlp_sv(translated_sent.replace('\\n', ' '))\n",
    "                tokens_trans = map(lambda tok: tok.text, spacy_doc_sv)\n",
    "            except:\n",
    "                print('not valid sentence: ', sentence)\n",
    "                translated_sent = sentence\n",
    "                tokens_trans = sentence\n",
    "\n",
    "\n",
    "            # spacy_doc_en = nlp_en(translated_sent.replace('\\n', ' '))\n",
    "            # tokens_trans = list(map(lambda token: token.text, spacy_doc_en))\n",
    "\n",
    "            all_tokens_org.append(tokens_org)\n",
    "            all_tokens_trans.append(tokens_trans)\n",
    "\n",
    "            if translated_sent.strip() and sentence.strip():\n",
    "                writer.write(sentence)\n",
    "                writer.write(' ||| ')\n",
    "                writer.write(' '.join(tokens_trans))\n",
    "                writer.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Add column with translated sentences and their lengths to df\n",
    "parallel_path = parallel_text_path\n",
    "all_sentence_len_org = sentence_lengths_dict\n",
    "all_sentence_len_trans = []\n",
    "all_texts_org = texts\n",
    "all_texts_trans = []\n",
    "with open(parallel_path, 'r') as reader:\n",
    "    for text in all_texts_org:\n",
    "        text_trans = []\n",
    "        sentence_len_trans = []\n",
    "        for i in range(len(text)):\n",
    "            line = reader.readline()\n",
    "            sentence_trans = line.split('|||')[1].split()\n",
    "            text_trans.append(sentence_trans)\n",
    "            sentence_len_trans.append(len(sentence_trans))\n",
    "        all_texts_trans.append(text_trans)\n",
    "        all_sentence_len_trans.append(sentence_len_trans)\n",
    "df['Text_trans'] = all_texts_trans\n",
    "df['Sentence_lengths_trans'] = all_sentence_len_trans\n",
    "display(df)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\"align_command = '../eflomal/align.py -i data/alignment/parallel.txt --priors data/alignment/sv-en2.priors --model 3 \\\\ -f data/alignment/suc.fwd -r data/alignment/suc.rev'\"\n",
    "'../eflomal/align.py -i data/alignment/parallel_small.txt --priors data/alignment/sv-en2.priors --model 3 \\\\ -f data/alignment/suc_small.fwd -r data/alignment/suc_small.rev'\"\n",
    "\"sym_command = '../word-alignment/python-test/fast_align/build/atools -c grow-diag-final-and -i data/alignment/suc.fwd -j data/alignment/suc.rev >data/alignment/suc.sym'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column with alignments per sentence and a dict with alignments in relation to the full text to df\n",
    "sym_file_path = alignment_sym_path\n",
    "all_sentence_len_org = df['Sentence_lengths']\n",
    "all_sentence_len_trans = df['Sentence_lengths_trans']\n",
    "all_formatted_alignments = []\n",
    "alignment_dicts = []\n",
    "with open(sym_file_path, 'r') as reader:\n",
    "    for sen_len_list, sen_len_list_trans in zip(all_sentence_len_org, all_sentence_len_trans):\n",
    "        formatted_alignments = []\n",
    "        alignment_dict = {}\n",
    "        offset_org = 0\n",
    "        offset_trans = 0\n",
    "        for sen_len, sen_len_trans in zip(sen_len_list, sen_len_list_trans):\n",
    "            line = reader.readline()\n",
    "            # line consists of alignment pairs in the form 0-0 1-1 2-1 ...\n",
    "            alignment_pairs = [list(map(int, pair.split('-'))) for pair in line.split()]\n",
    "            formatted_alignments.append(alignment_pairs)\n",
    "            for alignment_pair in alignment_pairs:\n",
    "                index_org = alignment_pair[0]\n",
    "                index_trans = alignment_pair[1]\n",
    "                alignment_dict.setdefault(index_org + offset_org, []).append(index_trans + offset_trans)\n",
    "                # alignment_dict[index_org + offset_org] = index_trans + offset_trans\n",
    "\n",
    "            offset_org += sen_len\n",
    "            offset_trans += sen_len_trans\n",
    "        all_formatted_alignments.append(formatted_alignments)\n",
    "        alignment_dicts.append(alignment_dict)\n",
    "df['Alignments'] = all_formatted_alignments\n",
    "df['Alignment_dict'] = alignment_dicts\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project the coref cluster from the original language\n",
    "# to the translated language\n",
    "cluster_lists_org = df['Coref_Clusters']\n",
    "sent_lengths_org = df['Sentence_lengths']\n",
    "sent_lengths_trans = df['Sentence_lengths_trans']\n",
    "alignment_dicts = df['Alignment_dict']\n",
    "cluster_lists_trans = []\n",
    "\n",
    "for idx, (clusters, alignment_dict, sent_len_org, sent_len_trans) in enumerate(zip(cluster_lists_org, alignment_dicts, sent_lengths_org, sent_lengths_trans)):\n",
    "    new_clusters = []\n",
    "    \n",
    "    for cluster in clusters:\n",
    "        new_cluster = []\n",
    "        # kolla span, mentions\n",
    "        try:\n",
    "            for [start_index, end_index] in cluster:\n",
    "                #print(cluster, df.loc[[idx]])\n",
    "                index_span = range(start_index, end_index + 1)\n",
    "                new_indices = []\n",
    "                for index in index_span:\n",
    "                    key = index\n",
    "                    if key in alignment_dict:\n",
    "                        new_indices.extend(alignment_dict.get(key))\n",
    "                if new_indices:\n",
    "                    new_cluster.append([min(new_indices), max(new_indices)])\n",
    "        except:\n",
    "            print(cluster, df.loc[[idx]])\n",
    "        new_clusters.append(new_cluster)\n",
    "    cluster_lists_trans.append(new_clusters)\n",
    "df['Coref_clusters_trans'] = cluster_lists_trans\n",
    "display(df)\n",
    "df.to_pickle(pkl_dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(input_list):\n",
    "    flat_list = [list_item for sublist in input_list for list_item in sublist]\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print words in the same cluster\n",
    "\n",
    "texts_org = df['Text']\n",
    "clusters_col_org = df['Coref_Clusters']\n",
    "texts_trans = df['Text_trans']\n",
    "clusters_col_trans = df['Coref_clusters_trans']\n",
    "\n",
    "for text_org, text, clusters_org, clusters in zip(texts_org[1100:1110], texts_trans[1100:1110], clusters_col_org[1100:1110], clusters_col_trans[1100:1110]):\n",
    "    flat_text = flatten(text)\n",
    "    flat_org = flatten(text_org)\n",
    "    print(' '.join(flat_text))\n",
    "    for cluster in clusters:\n",
    "        clustered_words = ''\n",
    "        for span in cluster:\n",
    "            [i_start, i_end] = span\n",
    "            clustered_words += str(flat_text[i_start:i_end+1]) + ', '\n",
    "        print(clustered_words)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Print words aligned with each other\n",
    "\n",
    "texts_org = df['Text']\n",
    "texts_trans = df['Text_trans']\n",
    "alignment_dicts = df['Alignment_dict']\n",
    "\n",
    "for text_org, text_trans, align_dict in zip(texts_org, texts_trans, alignment_dicts):\n",
    "    flat_text_org = flatten(text_org)\n",
    "    flat_text_trans = flatten(text_trans)\n",
    "    print(' '.join(flat_text_org))\n",
    "    print(' '.join(flat_text_trans))\n",
    "    for key, values in align_dict.items():\n",
    "        try:\n",
    "            if len(values) > 1:\n",
    "                print(flat_text_org[key], '\\t\\t\\t', [flat_text_trans[i] for i in values])\n",
    "        except:\n",
    "            print(\"Error in alignment:\", flat_text_org, key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check text lengths and remove long texts\n",
    "\n",
    "full_df = pd.read_pickle(pkl_dest_path)\n",
    "sentence_lengths = full_df['Sentence_lengths']\n",
    "count = 0\n",
    "rows_to_drop = []\n",
    "for idx, s in enumerate(sentence_lengths):\n",
    "    text_len = sum(s)\n",
    "    if text_len > 1000:\n",
    "       rows_to_drop.append(idx)\n",
    "       count+=1\n",
    "full_df.drop(full_df.index[rows_to_drop], inplace=True)\n",
    "display(full_df)\n",
    "full_df.to_pickle('../data/dataframes/coref_with_translations_1000.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#test\n",
    "t = \"Av naturen är tv-drama mycket lokaliserat.\"\n",
    "tokens_sv = nltk.word_tokenize(t)\n",
    "\n",
    "print(tokens_sv)\n",
    "\n",
    "spacy_doc_sv = nlp_sv(t)\n",
    "token_li = []\n",
    "for token in spacy_doc_sv:\n",
    "        if not token.text.isspace():\n",
    "            token_li.append(token.text)\n",
    "print(token_li)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}