{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.core.display import display\n",
    "from deep_translator import GoogleTranslator\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.sv import Swedish\n",
    "import os\n",
    "\n",
    "nlp_en = English()\n",
    "nlp_sv = Swedish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_pandas_display_options() -> None:\n",
    "    \"\"\"Set pandas display options.\"\"\"\n",
    "    # Ref: https://stackoverflow.com/a/52432757/\n",
    "    display = pd.options.display\n",
    "\n",
    "    display.max_columns = 100\n",
    "    display.max_rows = 10\n",
    "    display.max_colwidth = 400\n",
    "    display.width = 2000\n",
    "    # display.precision = 2  # set as needed\n",
    "\n",
    "set_pandas_display_options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print history of notebook\n",
    "%history -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# I suc core är datan uppdelad i meningar redan (all conll?), så det behövs inte göras.\n",
    "df = pd.read_pickle('../data/coref_ontonotes.pkl')\n",
    "texts = df['Text']\n",
    "all_tokens_org = []\n",
    "all_tokens_trans = []\n",
    "sentence_lengths_dict = df['Sentence_lengths']\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Deep translator takes a long time to run as it sleeps 2 seconds between every request\n",
    "with open('data/alignment/p', 'a') as writer:\n",
    "    for sentences in texts:\n",
    "        for tokens_org in sentences:\n",
    "            sentence = ' '.join(tokens_org)\n",
    "            try:\n",
    "                #translated_sents = GoogleTranslator('en', 'sv').translate_batch(joined_sents)\n",
    "                translated_sent = GoogleTranslator(source='en', target='sv').translate(text=sentence)\n",
    "                spacy_doc_sv = nlp_sv(translated_sent.replace('\\n', ' '))\n",
    "                tokens_trans = map(lambda token: token.text, spacy_doc_sv)\n",
    "            except:\n",
    "                print('not valid sentence: ', sentence)\n",
    "                translated_sent = sentence\n",
    "                tokens_trans = sentence\n",
    "\n",
    "\n",
    "            # spacy_doc_en = nlp_en(translated_sent.replace('\\n', ' '))\n",
    "            # tokens_trans = list(map(lambda token: token.text, spacy_doc_en))\n",
    "\n",
    "            all_tokens_org.append(tokens_org)\n",
    "            all_tokens_trans.append(tokens_trans)\n",
    "\n",
    "            if translated_sent.strip() and sentence.strip():\n",
    "                writer.write(sentence)\n",
    "                writer.write(' ||| ')\n",
    "                writer.write(' '.join(tokens_trans))\n",
    "                writer.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "parallel_path = '../data/alignment/parallel_small.txt'\n",
    "all_sentence_len_org = []\n",
    "all_sentence_len_trans = []\n",
    "with open(parallel_path, 'r') as reader:\n",
    "    lines = reader.readlines()\n",
    "    for line in lines:\n",
    "        sents = line.split('|||')\n",
    "        all_tokens_org.append(sents[0].split())\n",
    "        all_tokens_trans.append(sents[1].split())\n",
    "    # for tokens_sv in texts:\n",
    "    #     sentence = ' '.join(tokens_sv)\n",
    "    #     translated_sent = GoogleTranslator(source='sv', target='en').translate(text=sentence)\n",
    "    #     # spacy_doc_sv = nlp_sv(sent_sv.replace('\\\\n', ' '))\n",
    "    #     # tokens_sv = map(lambda token: token.text, spacy_doc_sv)\n",
    "    #     spacy_doc_en = nlp_en(translated_sent.replace('\\n', ' '))\n",
    "    #     tokens_en = list(map(lambda token: token.text, spacy_doc_en))\n",
    "    #     all_tokens_org.append(tokens_sv)\n",
    "    #     all_tokens_trans.append(tokens_en)\n",
    "    all_sentence_len_org = list(map(len, all_tokens_org))\n",
    "    all_sentence_len_trans = list(map(len, all_tokens_trans))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Add column with translated sentences and their lengths to df\n",
    "parallel_path = '../data/alignment/parallel_ontonotes.txt'\n",
    "all_sentence_len_org = sentence_lengths_dict\n",
    "all_sentence_len_trans = []\n",
    "all_texts_org = texts\n",
    "all_texts_trans = []\n",
    "with open(parallel_path, 'r') as reader:\n",
    "    for text in all_texts_org:\n",
    "        text_trans = []\n",
    "        sentence_len_trans = []\n",
    "        for i in range(len(text)):\n",
    "            line = reader.readline()\n",
    "            sentence_trans = line.split('|||')[1].split()\n",
    "            text_trans.append(sentence_trans)\n",
    "            sentence_len_trans.append(len(sentence_trans))\n",
    "        all_texts_trans.append(text_trans)\n",
    "        all_sentence_len_trans.append(sentence_len_trans)\n",
    "df['Text_trans'] = all_texts_trans\n",
    "df['Sentence_lengths_trans'] = all_sentence_len_trans\n",
    "display(df)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\"align_command = '../eflomal/align.py -i data/alignment/parallel.txt --priors data/alignment/sv-en2.priors --model 3 \\\\ -f data/alignment/suc.fwd -r data/alignment/suc.rev'\"\n",
    "'../eflomal/align.py -i data/alignment/parallel_small.txt --priors data/alignment/sv-en2.priors --model 3 \\\\ -f data/alignment/suc_small.fwd -r data/alignment/suc_small.rev'\"\n",
    "\"sym_command = '../word-alignment/python-test/fast_align/build/atools -c grow-diag-final-and -i data/alignment/suc.fwd -j data/alignment/suc.rev >data/alignment/suc.sym'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column with alignments per sentence and a dict with alignments in relation to the full text to df\n",
    "sym_file_path = '../data/alignment/ontonotes.sym'\n",
    "all_sentence_len_org = df['Sentence_lengths']\n",
    "all_sentence_len_trans = df['Sentence_lengths_trans']\n",
    "all_formatted_alignments = []\n",
    "alignment_dicts = []\n",
    "with open(sym_file_path, 'r') as reader:\n",
    "    for sen_len_list, sen_len_list_trans in zip(all_sentence_len_org, all_sentence_len_trans):\n",
    "        formatted_alignments = []\n",
    "        alignment_dict = {}\n",
    "        offset_org = 0\n",
    "        offset_trans = 0\n",
    "        for sen_len, sen_len_trans in zip(sen_len_list, sen_len_list_trans):\n",
    "            line = reader.readline()\n",
    "            # line consists of alignment pairs in the form 0-0 1-1 2-1 ...\n",
    "            alignment_pairs = [list(map(int, pair.split('-'))) for pair in line.split()]\n",
    "            formatted_alignments.append(alignment_pairs)\n",
    "            for alignment_pair in alignment_pairs:\n",
    "                index_org = alignment_pair[0]\n",
    "                index_trans = alignment_pair[1]\n",
    "                alignment_dict[index_org + offset_org] = index_trans + offset_trans \n",
    "\n",
    "            offset_org += sen_len\n",
    "            offset_trans += sen_len_trans\n",
    "        all_formatted_alignments.append(formatted_alignments)\n",
    "        alignment_dicts.append(alignment_dict)\n",
    "df['Alignments'] = all_formatted_alignments\n",
    "df['Alignment_dict'] = alignment_dicts\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project the coref cluster from the original language\n",
    "# to the translated language\n",
    "cluster_lists_org = df['Coref_Clusters']\n",
    "sent_lengths_org = df['Sentence_lengths']\n",
    "sent_lengths_trans = df['Sentence_lengths_trans']\n",
    "alignment_dicts = df['Alignment_dict']\n",
    "cluster_lists_trans = []\n",
    "\n",
    "for clusters, alignment_dict, sent_len_org, sent_len_trans in zip(cluster_lists_org, alignment_dicts, sent_lengths_org, sent_lengths_trans):\n",
    "    new_clusters = []\n",
    "    \n",
    "    for cluster in clusters:\n",
    "        new_cluster = []\n",
    "        # kolla span, mentions\n",
    "        for [start_index, end_index] in cluster:\n",
    "            index_span = range(start_index, end_index + 1)\n",
    "            new_indices = []\n",
    "            for index in index_span:\n",
    "                key = index\n",
    "                if key in alignment_dict:\n",
    "                    new_indices.append(alignment_dict.get(key))\n",
    "            if new_indices:\n",
    "                new_cluster.append([min(new_indices), max(new_indices)])\n",
    "        new_clusters.append(new_cluster)\n",
    "    cluster_lists_trans.append(new_clusters)\n",
    "df['Coref_clusters_trans'] = cluster_lists_trans\n",
    "display(df)\n",
    "df.to_pickle('../data/coref_with_translations.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(list):\n",
    "    flat_list = [item for sublist in list for item in sublist]\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print words in the same cluster\n",
    "\n",
    "texts_org = df['Text']\n",
    "clusters_col_org = df['Coref_Clusters']\n",
    "texts_trans = df['Text_trans']\n",
    "clusters_col_trans = df['Coref_clusters_trans']\n",
    "\n",
    "for text, clusters in zip(texts_trans, clusters_col_trans):\n",
    "    flat_text = flatten(text)\n",
    "    print(' '.join(flat_text))\n",
    "    for cluster in clusters:\n",
    "        clustered_words = ''\n",
    "        for span in cluster:\n",
    "            [i_start, i_end] = span\n",
    "            clustered_words += str(flat_text[i_start:i_end+1]) + ', '\n",
    "        print(clustered_words)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Print words aligned with each other\n",
    "\n",
    "texts_org = df['Text']\n",
    "texts_trans = df['Text_trans']\n",
    "alignment_dicts = df['Alignment_dict']\n",
    "\n",
    "for text_org, text_trans, align_dict in zip(texts_org, texts_trans, alignment_dicts):\n",
    "    flat_text_org = flatten(text_org)\n",
    "    flat_text_trans = flatten(text_trans)\n",
    "    for key, value in align_dict.items():\n",
    "        try:\n",
    "            print(flat_text_org[key], '\\t', flat_text_trans[value])\n",
    "        except:\n",
    "            print(\"Error in alignment:\", flat_text_org, key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}