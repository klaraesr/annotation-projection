{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "from IPython.core.display import display\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = \"../allennlp-model/data/train/train.english.v4_gold_conll\" # conll-formatted file to read into dataframe\n",
    "pkl_dest = 'data/dataframes/coref_ontonotes_train.pkl' # pickled dataframe destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "coref_df = pd.DataFrame(columns=['Doc_ID', 'Text_ID', 'Text', 'Sentence_lengths', 'Coref_Clusters'])\n",
    "print(coref_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_coref_clusters(path, document_id):\n",
    "    temp_df = pd.DataFrame(columns=['Doc_ID', 'Text_ID', 'Text', 'Sentence_lengths', 'Coref_Clusters'])\n",
    "\n",
    "    with open(path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "        sentence = []\n",
    "        text = []\n",
    "        sentence_lengths = []\n",
    "        coref_clusters = {}\n",
    "        temp_clusters = {}\n",
    "        columns = []\n",
    "\n",
    "        for line in lines:\n",
    "            # if we enter a new document\n",
    "            if line.startswith('#'):\n",
    "                if text:\n",
    "                    document_id = columns[0]\n",
    "                    text_id = columns[1]\n",
    "                    temp_df = temp_df.append({'Doc_ID': document_id,\n",
    "                                              'Text_ID': document_id + '-' + str(text_id),\n",
    "                                              'Text': text,\n",
    "                                              'Sentence_lengths': sentence_lengths,\n",
    "                                              'Coref_Clusters': list(coref_clusters.values())},\n",
    "                                             ignore_index=True)\n",
    "                    text = []\n",
    "                    sentence_lengths = []\n",
    "                    coref_clusters = {}\n",
    "                    temp_clusters = {}\n",
    "            elif line == '\\n':\n",
    "                text.append(sentence)\n",
    "                sentence_lengths.append(int(columns[2]) + 1) #zero indexed in dataset\n",
    "                sentence = []\n",
    "            else:\n",
    "                columns = line.split()\n",
    "                document_id = columns[0]\n",
    "                index = int(columns[2]) + sum(sentence_lengths)\n",
    "                sentence.append(columns[3])\n",
    "                coref_column = columns[-1]\n",
    "                if not coref_column == '-':\n",
    "                    corefs = coref_column.split('|')\n",
    "                    for coref in corefs:\n",
    "                        key = coref.strip('()')\n",
    "                        if re.match(r'\\([0-9]+\\)', coref):\n",
    "                            coref_clusters.setdefault(key, []).append([index, index])\n",
    "                        if re.match(r'^\\([0-9]+$', coref):\n",
    "                            temp_clusters.setdefault(key, []).append(index)\n",
    "                        if re.match(r'^[0-9]+\\)$', coref):\n",
    "                            temp_clusters.setdefault(key, []).append(index)\n",
    "                            cluster_span = temp_clusters.pop(key)\n",
    "                            # om det finns fler än två index i cluster_span\n",
    "                            # spara det andra i temp cluster\n",
    "                            if len(cluster_span) > 2:\n",
    "                                temp_clusters.setdefault(key, []).append(cluster_span.pop(1))\n",
    "                            coref_clusters.setdefault(key, []).append(cluster_span)\n",
    "    display(temp_df)\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#for path in Path(data_path).rglob('*.gold_conll'):\n",
    "# with open(data_path, 'r') as reader:\n",
    "#     print(path)\n",
    "#     filename = str(path).split('/')[-1]\n",
    "    #document_id = filename.split('.')[0]\n",
    "df = get_coref_clusters(data_path, 'rand')\n",
    "coref_df = coref_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#display(coref_df)\n",
    "save_ids = ('wsj_2358', 'wsj_2346', 'wsj_2356', 'wsj_2321', 'wsj_2316', 'wsj_2350', 'wsj_2363', 'wsj_2309', 'wsj_2354', 'wsj_2344', 'wsj_2383', 'c2e_0029', 'c2e_0039', 'p2.5_a2e_0039', 'p2.5_c2e_0049', 'c2e/00/c2e_0009', '42/nt_4209', '40/nt_4019', '10/ot_1019', '45/nt_4509', '12/ot_1209', '11/ot_1119', '41/nt_4109', '58/nt_5809', 'sinorama/10/ectb_1069', 'xinhua/01/chtb_0129', '/xinhua/01/chtb_0109')\n",
    "#print(coref_df.loc[df['Doc_ID'].contains('00/c2e_0009')].loc[331, 'Text'])\n",
    "df2 = coref_df.loc[coref_df['Doc_ID'].str.endswith(save_ids)]\n",
    "#df2 = coref_df.loc[any(ext in url_string for ext in extensionsToCheck)]\n",
    "display(df2)\n",
    "coref_df.to_pickle(pkl_dest)\n",
    "df2.to_pickle('../data/dataframes/eng_suc_sim')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}